{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8410a731",
   "metadata": {},
   "source": [
    "# RegGenome challenge\n",
    "\n",
    "RegGenome have trained a regulatory / non-regulatory (RNR) classifier based on the language used in the text of documents published by legislative bodies.\n",
    "\n",
    "The classifier has performed very well in development, scoring over 99% accuracy.\n",
    "\n",
    "However, after the model was deployed to production many non-regulatory documents started being misclassified as regulatory. The new publisher web crawlers have been optimised for high recall and the large number of spurious documents is causing high load on downstream systems and analysts.\n",
    "\n",
    "Below is the notebook which builds and tests the model. Please take an hour to consider the following questions:\n",
    "\n",
    "1. Why is the model performing worse in a production setting?\n",
    "2. How could we have predicted this?\n",
    "3. What strategies could we employ to improve the performance? Please consider:\n",
    "    a) Technical / DS strategies\n",
    "    b) Non-technical / organisational strategies\n",
    "\n",
    "Please edit or add to the notebook to demonstrate technical strategies for 2 & 3a. Due to time constraints they do not need to be fully formed but should demonstrate your programming ability and a grasp of the issues involved.\n",
    "\n",
    "**Please spend no more than an hour on this challenge.**\n",
    "\n",
    "We will spend 20-30 minutes of our interview discussing your proposals. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pd.__version__))\n",
    "print('Numpy: {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('rnr-examples.csv', sep=\",\", header=0, encoding='utf-8')\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text']\n",
    "labels = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b005008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create bag of words\n",
    "all_words = []\n",
    "\n",
    "for text in texts:\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Inspect the total number of words and the 15 most common words\n",
    "\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f33bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 1,000 most common words as features\n",
    "\n",
    "word_features = list(all_words.keys())[:1000]\n",
    "\n",
    "def find_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the features for all the texts\n",
    "\n",
    "texts = list(zip(texts, labels))\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(texts)\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "feature_sets = [(find_features(text), label) for (text, label) in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest classifier\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = SklearnClassifier(RandomForestClassifier())\n",
    "model.train(feature_sets)\n",
    "accuracy = nltk.classify.accuracy(model, feature_sets)*100\n",
    "\n",
    "print(\"Classifier Accuracy: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
